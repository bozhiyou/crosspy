{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why CrossPy?\n",
    "\n",
    "Heterogeneous architectures, typically consisting of CPU and GPU-based systems, have become ubiquitous in clusters for scientific computing and machine learning.\n",
    "To harness the power of these architectures, libraries and packages have been developed in Python, the dominant programming language for scientific computing applications. [NumPy][1]/[SciPy][2] has emerged as a fundamental library for scientific computing on CPU hosts, while [CuPy][3] has been developed for GPU accelerators.\n",
    "Although they work efficiently with respect to the specific architecture, the challenge of programming with a mix of the libraries is left to the programmer.\n",
    "\n",
    "[1]: https://numpy.org/doc/stable/index.html\n",
    "[2]: https://scipy.org\n",
    "[3]: https://docs.cupy.dev/en/stable/index.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of array addition. On a CPU host, we use [NumPy][1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.40865536 0.7452975  1.84086731 0.37142214 1.38734922 1.0010276\n",
      " 0.54878015 0.24301411 1.06460233 1.82873078]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(10)\n",
    "b = np.random.rand(10)\n",
    "c = a + b\n",
    "print(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple. By replacing `numpy` with `cupy`, we get a single-GPU implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4626043  0.52709642 1.44451667 0.50461193 1.22073328 0.75054192\n",
      " 1.0051937  1.02946935 1.89002751 1.57725468]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "with cp.cuda.Device(0):\n",
    "    ag = cp.random.rand(10)\n",
    "    bg = cp.random.rand(10)\n",
    "    cg = ag + bg\n",
    "print(cg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple, too. Well, what if we want to (or have to when the array is too large to reside on a single device) make use of multiple devices?\n",
    "Say, half computation with a CPU and half with a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.28334521 1.08025706 0.96608659 0.86479402 1.01503959 0.91012578\n",
      " 0.60952607 0.66134588 0.44273132 1.08505553]\n"
     ]
    }
   ],
   "source": [
    "dummy_large_number = 10\n",
    "\n",
    "# conceptually\n",
    "a_origin = np.random.rand(dummy_large_number)\n",
    "b_origin = np.random.rand(dummy_large_number)\n",
    "\n",
    "a_first_half = a_origin[:dummy_large_number // 2]\n",
    "b_first_half = b_origin[:dummy_large_number // 2]\n",
    "c_first_half = a_first_half + b_first_half\n",
    "\n",
    "with cp.cuda.Device(0):\n",
    "    a_second_half = cp.asarray(a_origin[dummy_large_number // 2:])\n",
    "    b_second_half = cp.asarray(b_origin[dummy_large_number // 2:])\n",
    "    c_second_half = a_second_half + b_second_half\n",
    "\n",
    "c = np.concatenate((c_first_half, cp.asnumpy(c_second_half)))\n",
    "print(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already looks cumbersome. Similarly, we can use two GPUs with minimal changes to the example above but without reducing the complexity of programming."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how CrossPy programs this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array {((0, 5),): array([1.28334521, 1.08025706, 0.96608659, 0.86479402, 1.01503959]), ((5, 10),): array([0.91012578, 0.60952607, 0.66134588, 0.44273132, 1.08505553])}\n"
     ]
    }
   ],
   "source": [
    "import crosspy as xp\n",
    "\n",
    "ax = xp.array([a_first_half, a_second_half], axis=0)\n",
    "bx = xp.array([b_first_half, b_second_half], axis=0)\n",
    "cx = ax + bx\n",
    "print(cx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As simple as the first example! CrossPy handles the complexity of cross-device manipulation and thus eliminates the burden of tedious programming. The printed `cx` is a CrossPy array across two devices where the range `[0:5]` is on one and `[5:10]` is on the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "{((0, 5),): 'cpu', ((5, 10),): <CUDA Device 0>}\n"
     ]
    }
   ],
   "source": [
    "print(cx.shape)\n",
    "print(cx.device_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also start from the original large array and let CrossPy handle the partitioning, without adding to the lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array {((0, 5),): array([1.28334521, 1.08025706, 0.96608659, 0.86479402, 1.01503959]), ((5, 10),): array([0.91012578, 0.60952607, 0.66134588, 0.44273132, 1.08505553])}\n"
     ]
    }
   ],
   "source": [
    "from crosspy import cpu, gpu\n",
    "\n",
    "ax = xp.array(a_origin, distribution=[cpu(0), gpu(0)], axis=0)\n",
    "bx = xp.array(b_origin, distribution=[cpu(0), gpu(0)], axis=0)\n",
    "cx = ax + bx\n",
    "print(cx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows the power of CrossPy that, for a single-device implementation to migrate to a multi-device version, we only need to initialize the arrays with CrossPy and specify how we want the array to be distributed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we highlight an incomplete list of CrossPy features:\n",
    "\n",
    "- [Flexible data distribution (including uniform partitioning and arbitrary coloring)](data_exchange)\n",
    "\n",
    "- [Distribution-agnostic data transfer interfaces](partitioning)\n",
    "\n",
    "\n",
    "More details can be found in this documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
